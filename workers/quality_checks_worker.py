import re
from workers.base_worker import BaseWorker
from utils.mindfile import Mindfile
from ai_service import get_ai_response
from utils.prompt_utils import build_initial_conversation_history

class QualityChecksWorker(BaseWorker):
    def __init__(self, mindfile: Mindfile):
        super().__init__("quality_checks_worker")
        self.mindfile = mindfile

    def _construct_prompt(self, conversation_history: list[dict[str, str]], original_answer: str) -> str:
        # TODO: Add a prompt for factual correctness and style similarity, after we implemented them.
        prompt = f"""
            Our goal is to evaluate the quality of a response generated by another (experimental) instance of you.
            Please analyze the provided conversation history and the original answer below, and then rate the answer on the following criteria on a scale from 1 to 10.
            
            Criteria:
            - sys_message_compliance: Does the answer adhere to the persona and instructions defined in the system message?
            - self_description_correctness: Is the answer consistent with the provided self-facts?

            Often, the answer is not really good, not really bad. Thus, use the entire scale, not just 1 or 10, unless it's a very clear case. 
            This is important, because our system will compare the scores of different answers, to select the best one.

            Sometimes, the answer is fully consistent with the self-facts, but fails to fully adhere to the system message. Sometimes it's the other way around.
            Thus, we need to evaluate them separately and independently.
            
            You can use this scale for each:

            1 - A complete fail.
            2 -
            3 - Mostly bad.
            4 -
            5 - Meh.
            6 -
            7 - Ok, but with some issues.
            8 -
            9 -
            10 - Perfect.

            A few tips for evaluating sys_message_compliance:
            - Split the system message into logical parts, and check each against the original answer.
            - List all the imperfections and errors of the original answer. 
            - Formatting, style, text length - do matter. Does the original answer look like it was written by the persona or by an LLM?

            We need an objective and honest assessment. Remember the stakes.  
            It's not enough for the answer to be recognizable as coming from the persona. We must critically evaluate it, detect all imperfections.

            Conversation History:
            "{conversation_history}"

            Original Answer to Evaluate:
            '{original_answer}'

            Please be very concise.
            
            Please write a short verdict (free form) on the quality of the answer.

            And then provide your ratings in the following format (this will be parsed automatically):
            sys_message_compliance: [numerical_score]
            self_description_correctness: [numerical_score]
        """
        return prompt

    def process(self, conversation_history: list[dict[str, str]], original_answer: str, user_info_prompt: str | None = None) -> dict[str, int | None]:
        if not original_answer or not original_answer.strip():
            print("QualityChecksWorker: Received empty answer to check. Returning None.")
            return {
                "sys_message_compliance": None,
                "self_description_correctness": None,
                #"factual_correctness": None, # disabled for now
                #"style_similarity": None, # disabled for now
            }

        # Build the context specific to this worker
        worker_context = self.mindfile.get_context(self.mindfile_parts)
        system_message = self.mindfile.get_system_message()
        
        if user_info_prompt:
            system_message += "\n\n" + user_info_prompt

        quality_prompt = self._construct_prompt(conversation_history, original_answer)

        llm_conversation_history = build_initial_conversation_history(
            system_message=system_message,
            context=worker_context,
            user_prompt=quality_prompt
        )

        # We don't need the provider report for this internal call.
        raw_quality_assessment, _ = get_ai_response(
            messages_history=llm_conversation_history,
            user_input_for_provider_selection=quality_prompt, # The prompt itself is used for provider selection logic
            max_length=1000 
        )

        print(f"QualityChecksWorker: Raw quality assessment: {raw_quality_assessment}")

        return self._parse_scores(raw_quality_assessment)

    def _parse_scores(self, assessment_text: str) -> dict[str, int | None]:
        scores = {
            "sys_message_compliance": None,
            "self_description_correctness": None,
            #"factual_correctness": None, # disabled for now
            #"style_similarity": None, # disabled for now
        }
        
        if not assessment_text:
            return scores

        # Regex to find key-score pairs, allowing for different separators and optional whitespace.
        # It looks for lines starting with one of the score keys.
        for key in scores.keys():
            match = re.search(fr"^{key}\s*[:\-]\s*(\d+)", assessment_text, re.IGNORECASE | re.MULTILINE)
            if match:
                try:
                    score = int(match.group(1))
                    scores[key] = max(1, min(10, score)) # Clamp score between 1 and 10
                except (ValueError, IndexError):
                    print(f"QualityChecksWorker: Could not parse score for '{key}' from assessment.")
                    
        return scores

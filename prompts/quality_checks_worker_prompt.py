from config import ANSWER_TO_USER_TAG, CHAIN_OF_THOUGHT_TAG


def construct_prompt(
    conversation_history: list[dict[str, str]], original_answer: str
) -> str:
    # TODO: Add a prompt for factual correctness and style similarity, after we implemented them.
    prompt = f"""
            Our goal is to evaluate the quality of a response generated by another (experimental) instance of you.
            Please analyze the provided conversation history and the original answer below, and then rate the answer on the following criteria on a scale from 1 to 10.
            
            Criteria:
            - sys_message_compliance: Does the answer adhere to the persona and instructions defined in the system message?
            - self_description_correctness: Is the answer consistent with the provided self-facts?

            Often, the answer is not really good, not really bad. Thus, use the entire scale, not just 1 or 10, unless it's a very clear case. 
            This is important, because our system will compare the scores of different answers, to select the best one.

            Sometimes, the answer is fully consistent with the self-facts, but fails to fully adhere to the system message. Sometimes it's the other way around.
            Thus, we need to evaluate them separately and independently.
            
            You can use this scale for each:

            1 - A complete fail.
            2 -
            3 - Mostly bad.
            4 -
            5 - Meh.
            6 -
            7 - Ok, but with some issues.
            8 -
            9 -
            10 - Perfect.
            
            Only rare exceptionally good answers deserve the 10. If you're thinking about assigning 10, double-check that the answer fully passes the high standards of yours. 
            You are a strict and meticulous judge of quality. We are striving for perfection here. For every minor or major issue with the answer, take away at least one point. 


            A few tips for evaluating sys_message_compliance:
            - Split the system message into logical parts, and check each against the original answer.
            - List all the imperfections and errors of the original answer. 
            - Formatting, style, text length - do matter. Does the original answer look like it was written by the persona or by an LLM?

            We need an objective and honest assessment. Remember the stakes.  
            It's not enough for the answer to be recognizable as coming from the persona. We must critically evaluate it, detect all imperfections.

            Evaluate the adherence to every part of the system message. 

            <useful heuristics>
            - Does it combine answers to several messages / different people? This is a severe violation of the system message, minus 5 points.
            - Is it answering to an old message? (the answer should always be to the LAST message of the conversation). A severe sys msg violation, minus 5 points.
            - You can ignore issues in the answer parts that are not facing the user (e.g. the '{CHAIN_OF_THOUGHT_TAG}' part).
            </useful heuristics>

            Note: focus on the user-facing part of the answer (the one between '{ANSWER_TO_USER_TAG}' tags).
            If other parts (e.g. the '{CHAIN_OF_THOUGHT_TAG}') have formatting issues etc, you can ignore them.

            Conversation History:
            "{conversation_history}"

            Original Answer to Evaluate:
            '{original_answer}'  

            You can use the following answer template:

            <answer_template>
            [A short verdict (free form) on the quality of the answer. Please be very concise here.]
            Ratings:
            sys_message_compliance: [numerical_score]
            self_description_correctness: [numerical_score]
            </answer_template>
            
            <important notes>
            This is of the critical importance:
            - Our job here is meta. We don't write an answer to a participant of a chat. Instead, we evaluate the answer supplied to us by another instance. We are the quality control team.
            - Your answer will be parsed automatically, so please adhear to the answer template.
            </important notes>

            Your answer:
        """
    return prompt

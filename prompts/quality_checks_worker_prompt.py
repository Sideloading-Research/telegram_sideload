from config import ANSWER_TO_USER_TAG, CHAIN_OF_THOUGHT_TAG


def construct_prompt(
    conversation_history: list[dict[str, str]], original_answer: str
) -> str:
    # TODO: Add a prompt for factual correctness and style similarity, after we implemented them.
    prompt = f"""
            Our goal is to evaluate the quality of a response generated by another (experimental) instance of you.
            Please analyze the provided conversation history and the original answer below, and then rate the answer on the following criteria on a scale from 1 to 10.
            
            Criteria:
            - sys_message_compliance: Does the answer adhere to the persona and instructions defined in the system message?
            - self_description_correctness: Is the answer consistent with the provided self-facts?

            Often, the answer is not really good, not really bad. Thus, use the entire scale, not just 1 or 10, unless it's a very clear case. 
            This is important, because our system will compare the scores of different answers, to select the best one.

            Sometimes, the answer is fully consistent with the self-facts, but fails to fully adhere to the system message. Sometimes it's the other way around.
            Thus, we need to evaluate them separately and independently.
            
            You can use this scale for each:

            1 - A complete fail.
            2 -
            3 - Mostly bad.
            4 -
            5 - Meh.
            6 -
            7 - Ok, but with some issues.
            8 -
            9 -
            10 - Perfect.
            
            Only rare exceptionally good answers deserve the 10. If you're thinking about assigning 10, double-check that the answer fully passes the high standards of yours. 

            A few tips for evaluating sys_message_compliance:
            - Split the system message into logical parts, and check each against the original answer.
            - List all the imperfections and errors of the original answer. 
            - Formatting, style, text length - do matter. Does the original answer look like it was written by the persona or by an LLM?

            We need an objective and honest assessment. Remember the stakes.  
            It's not enough for the answer to be recognizable as coming from the persona. We must critically evaluate it, detect all imperfections.

            Note: focus on the user-facing part of the answer (the one between '{ANSWER_TO_USER_TAG}' tags).
            If other parts (e.g. the '{CHAIN_OF_THOUGHT_TAG}') have formatting issues etc, you can ignore them.

            Conversation History:
            "{conversation_history}"

            Original Answer to Evaluate:
            '{original_answer}'  

            You can use the following answer template:

            <answer_template>
            [A short verdict (free form) on the quality of the answer. Please be very concise here.]
            Ratings:
            sys_message_compliance: [numerical_score]
            self_description_correctness: [numerical_score]
            </answer_template>
            
            Note: your answer will be parsed automatically, so please adhear to the answer template.
            
            Your answer:
        """
    return prompt
